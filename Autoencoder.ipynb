{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "image_length = 784\n",
    "encoding_dim = 200\n",
    "encoding_dim_2 = 45\n",
    "\n",
    "train_size = 5000\n",
    "validation_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and convert data to percentage points (divide by 255, the max gray scale value)\n",
    "x_train = pd.read_csv(\"data/mnist_train.csv\", header = None)\n",
    "y_train = x_train.loc[:, 0].to_numpy()\n",
    "x_train = x_train.loc[:, 1:].to_numpy()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "\n",
    "x_test = pd.read_csv(\"data/mnist_test.csv\", header = None)\n",
    "y_test = x_test.loc[:, 0].to_numpy()\n",
    "x_test = x_test.loc[:, 1:].to_numpy()\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "# Take samples for training and testing. Note that we cannot take the test set yet, as we will use this when the\n",
    "# full network is completed in R\n",
    "x_validation = x_train[train_size:(train_size+validation_size), :]\n",
    "x_train_input = x_train[(train_size+validation_size):, :]\n",
    "x_train = x_train[:train_size, :]\n",
    "y_train_input = y_train[(train_size+validation_size):]\n",
    "\n",
    "# Create one-hot-encoded versions of the data\n",
    "y_train_input_ohe = to_categorical(y_train_input)\n",
    "y_test_ohe = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mikew\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Create autoencoder with topology 784 -> 200 -> 45\n",
    "input_img = Input(shape = (image_length,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "encoded = Dense(encoding_dim_2, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "decoded = Dense(image_length, activation='sigmoid')(decoded)\n",
    "\n",
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 12\n",
      "Batch size: 64\n",
      "WARNING:tensorflow:From C:\\Users\\mikew\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Batch size: 128\n",
      "Epochs: 13\n",
      "Batch size: 64\n",
      "Batch size: 128\n",
      "Epochs: 14\n",
      "Batch size: 64\n"
     ]
    }
   ],
   "source": [
    "# Grid Search - after finding epochs=12 and batch_size=128 was the best\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(12, 20):\n",
    "    print(f\"Epochs: {epoch}\")\n",
    "    for batch_size in [64, 128]:\n",
    "        print(f\"Batch size: {batch_size}\")\n",
    "        \n",
    "        autoencoder = Model(input_img, decoded)\n",
    "        autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        history = autoencoder.fit(x_train, x_train,\n",
    "                          epochs=epoch, batch_size=batch_size,\n",
    "                          shuffle=True, verbose = 0,\n",
    "                          validation_data=(x_validation, x_validation))\n",
    "        \n",
    "        if (history.history['loss'][-1] > history.history['val_loss'][-1]):\n",
    "            print(\"Overfitting!\")\n",
    "            next\n",
    "            \n",
    "        validation_acc = history.history['val_acc'][-1]\n",
    "        \n",
    "        if (validation_acc > best_acc):\n",
    "            best_acc = validation_acc\n",
    "            best_epoch = epoch\n",
    "            best_batch_size = batch_size\n",
    "            \n",
    "print(f\"Best accuracy: {best_acc}, best epochs: {best_epoch}, best batch_size: {best_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best parameters found above\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "history = autoencoder.fit(x_train, x_train,\n",
    "                          epochs=best_epoch, batch_size=best_batch_size,\n",
    "                          shuffle=True, verbose = 2,\n",
    "                          validation_data=(x_validation, x_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot images\n",
    "encoded_images = encoder.predict(x_train_input)\n",
    "decoded_images = autoencoder.predict(x_train_input)\n",
    "\n",
    "plt.figure(figsize=(40, 4))\n",
    "for i in range(10):\n",
    "    # Display original images\n",
    "    ax = plt.subplot(3, 20, i + 1)\n",
    "    plt.imshow(x_train_input[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    # Display encoded images\n",
    "    ax = plt.subplot(3, 20, i + 1 + 20)\n",
    "    plt.imshow(encoded_images[i].reshape(9,5))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    # Display decoded images\n",
    "    ax = plt.subplot(3, 20, 2*20 +i+ 1)\n",
    "    plt.imshow(decoded_images[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the autoencoding part is done, we add the ANN part from exercise 4\n",
    "full_model = Model(autoencoder.input, autoencoder.layers[-3].output)\n",
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 20 # H = 20 from exercise 4\n",
    "\n",
    "layer_1 = Dense(H, activation='relu')(full_model.layers[-1].output)\n",
    "layer_2 = Dense(10, activation = 'softmax')(layer)\n",
    "full_model = Model(input_img, layer_2)\n",
    "full_model.summary()\n",
    "full_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = full_model.fit(x_train_input, y_train_input_ohe,\n",
    "                         epochs=epoch, batch_size=batch_size,\n",
    "                         shuffle=True, verbose=2,\n",
    "                         validation_data=(x_test, y_test_ohe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
